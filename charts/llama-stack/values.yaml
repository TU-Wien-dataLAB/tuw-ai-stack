# Note: The starter distribution uses built-in configuration
# All providers are configured via environment variables below

distribution: distribution-starter

# VLLM configuration
vllm:
  url: "http://vllm-prod-router-service.vllm.svc.cluster.local/v1"

# Milvus configuration (inline - no external service needed)
milvus: {}
  # Inline Milvus uses embedded storage, no connection settings required

# Telemetry configuration
telemetry:
  enabled: false
  serviceName: "otel-collector.openshift-opentelemetry-operator.svc.cluster.local:4318"
  sinks: "console,sqlite,otel"

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

#  Use to allow for other env variables to be passed to the container
#  Use to allow for other env variables to be passed to the container
# env:
#   MY_CUSTOM_ENV_VAR: "my-custom-env-var-value"

replicaCount: 1

# This sets the container image more information can be found here: https://kubernetes.io/docs/concepts/containers/images/
image:
  repository: ghcr.io/tu-wien-datalab/llama-stack
  tag: main
  # This sets the pull policy for images.
  pullPolicy: Always

# This section builds out the service account more information can be found here: https://kubernetes.io/docs/concepts/security/service-accounts/
serviceAccount:
  # Specifies whether a service account should be created
  create: false
  # Automatically mount a ServiceAccount's API credentials?
  automount: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

# This is for setting Kubernetes Annotations to a Pod.
# For more information checkout: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
podAnnotations: {}
# This is for setting Kubernetes Labels to a Pod.
# For more information checkout: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
podLabels: {}

podSecurityContext:
  fsGroup: 1000
  runAsUser: 1000
  runAsGroup: 1000

# This is for setting up a service more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/
service:
  # This sets the service type more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types
  type: ClusterIP
  # This sets the ports more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#field-spec-ports
  port: 5001

# Deployment strategy configuration
strategy:
  # Set to recreate for clean restarts with state changes
  type: Recreate
  # rollingUpdate:
  #   maxSurge: 25%
  #   maxUnavailable: 25%

# Persistence configuration
persistence:
  enabled: true
  storageClass: "" # Use default storage class
  accessModes:
    - ReadWriteOnce
  size: 20Gi
  mountPath: "/home/lls/.llama"

# Additional volumes on the output Deployment definition.
volumes: []
# - name: foo
#   secret:
#     secretName: mysecret
#     optional: false

# Additional volumeMounts on the output Deployment definition.
volumeMounts: []
# - name: foo
#   mountPath: "/etc/foo"
#   readOnly: true

# This block is for setting up the ingress for more information can be found here: https://kubernetes.io/docs/concepts/services-networking/ingress/
ingress:
  enabled: false
  className: ""
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources:
  limits:
    cpu: 100m
    memory: 500Mi
  requests:
    cpu: 100m
    memory: 500Mi

# This is to setup the liveness and readiness probes more information can be found here: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
livenessProbe:
  httpGet:
    path: /v1/health
    port: 5001
readinessProbe:
  httpGet:
    path: /v1/health
    port: 5001
startupProbe:
  httpGet:
    path: /v1/health
    port: 5001
  initialDelaySeconds: 40
  periodSeconds: 10
  failureThreshold: 30

# Llama Stack run configuration
# This is the complete run.yaml configuration that will be mounted as a ConfigMap
runConfig:
  version: 2
  image_name: tuw-ai
  apis:
    - agents
    - batches
    - files
    - inference
    - safety
    - tool_runtime
    - vector_io
  providers:
    inference:
      - provider_id: ${env.VLLM_URL:+vllm}
        provider_type: remote::vllm
        config:
          url: ${env.VLLM_URL:=}
          max_tokens: ${env.VLLM_MAX_TOKENS:=4096}
          api_token: ${env.VLLM_API_TOKEN:=fake}
          tls_verify: ${env.VLLM_TLS_VERIFY:=true}
          allowed_models: []
      - provider_id: openai
        provider_type: remote::openai
        config:
          api_key: ${env.OPENAI_API_KEY:=}
          base_url: ${env.OPENAI_BASE_URL:=https://api.openai.com/v1}
    vector_io:
      - provider_id: faiss
        provider_type: inline::faiss
        config:
          persistence:
            namespace: vector_io::faiss
            backend: kv_default
      - provider_id: sqlite-vec
        provider_type: inline::sqlite-vec
        config:
          db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/sqlite_vec.db
          persistence:
            namespace: vector_io::sqlite_vec
            backend: kv_default
      - provider_id: ${env.MILVUS_URL:+milvus}
        provider_type: inline::milvus
        config:
          db_path: ${env.MILVUS_DB_PATH:=~/.llama/distributions/starter}/milvus.db
          persistence:
            namespace: vector_io::milvus
            backend: kv_default
      - provider_id: ${env.CHROMADB_URL:+chromadb}
        provider_type: remote::chromadb
        config:
          url: ${env.CHROMADB_URL:=}
          persistence:
            namespace: vector_io::chroma_remote
            backend: kv_default
      - provider_id: ${env.PGVECTOR_DB:+pgvector}
        provider_type: remote::pgvector
        config:
          host: ${env.PGVECTOR_HOST:=localhost}
          port: ${env.PGVECTOR_PORT:=5432}
          db: ${env.PGVECTOR_DB:=}
          user: ${env.PGVECTOR_USER:=}
          password: ${env.PGVECTOR_PASSWORD:=}
          persistence:
            namespace: vector_io::pgvector
            backend: kv_default
      - provider_id: ${env.QDRANT_URL:+qdrant}
        provider_type: remote::qdrant
        config:
          api_key: ${env.QDRANT_API_KEY:=}
          persistence:
            namespace: vector_io::qdrant_remote
            backend: kv_default
    files:
      - provider_id: meta-reference-files
        provider_type: inline::localfs
        config:
          storage_dir: ${env.FILES_STORAGE_DIR:=~/.llama/distributions/starter/files}
          metadata_store:
            table_name: files_metadata
            backend: sql_default
    safety:
      - provider_id: llama-guard
        provider_type: inline::llama-guard
        config:
          excluded_categories: []
      - provider_id: code-scanner
        provider_type: inline::code-scanner
    agents:
      - provider_id: meta-reference
        provider_type: inline::meta-reference
        config:
          persistence:
            agent_state:
              namespace: agents
              backend: kv_default
            responses:
              table_name: responses
              backend: sql_default
              max_write_queue_size: 10000
              num_writers: 4
    tool_runtime:
      - provider_id: brave-search
        provider_type: remote::brave-search
        config:
          api_key: ${env.BRAVE_SEARCH_API_KEY:=}
          max_results: 3
      - provider_id: tavily-search
        provider_type: remote::tavily-search
        config:
          api_key: ${env.TAVILY_SEARCH_API_KEY:=}
          max_results: 3
      - provider_id: rag-runtime
        provider_type: inline::rag-runtime
      - provider_id: model-context-protocol
        provider_type: remote::model-context-protocol
    batches:
      - provider_id: reference
        provider_type: inline::reference
        config:
          kvstore:
            namespace: batches
            backend: kv_default
  storage:
    backends:
      kv_default:
        type: kv_sqlite
        db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/kvstore.db
      sql_default:
        type: sql_sqlite
        db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/sql_store.db
    stores:
      metadata:
        namespace: registry
        backend: kv_default
      inference:
        table_name: inference_store
        backend: sql_default
        max_write_queue_size: 10000
        num_writers: 4
      conversations:
        table_name: openai_conversations
        backend: sql_default
      prompts:
        namespace: prompts
        backend: kv_default
  registered_resources:
    models:
      - model_id: nomic-ai/nomic-embed-text-v1.5
        provider_id: sentence-transformers
        provider_model_id: nomic-ai/nomic-embed-text-v1.5
        model_type: embedding
        metadata:
          embedding_dimension: 768
    shields:
      - shield_id: llama-guard
        provider_id: ${env.SAFETY_MODEL:+llama-guard}
        provider_shield_id: ${env.SAFETY_MODEL:=}
      - shield_id: code-scanner
        provider_id: ${env.CODE_SCANNER_MODEL:+code-scanner}
        provider_shield_id: ${env.CODE_SCANNER_MODEL:=}
    vector_dbs: []
    datasets: []
    scoring_fns: []
    benchmarks: []
    tool_groups:
      - toolgroup_id: builtin::websearch
        provider_id: tavily-search
      - toolgroup_id: builtin::rag
        provider_id: rag-runtime
  server:
    port: 8321
  vector_stores:
    default_provider_id: faiss
    default_embedding_model:
      provider_id: sentence-transformers
      model_id: nomic-ai/nomic-embed-text-v1.5
  safety:
    default_shield_id: llama-guard
